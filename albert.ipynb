{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974999f-d103-4bed-b1cd-1269649e3603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AlbertTokenizer, AlbertForQuestionAnswering\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2', use_fast=True)\n",
    "\n",
    "def load_spoken_squad_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    contexts = []\n",
    "    answers = []\n",
    "    \n",
    "    for item in data['data']:\n",
    "        for paragraph in item['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    answers.append({\n",
    "                        'text': answer['text'],\n",
    "                        'answer_start': answer['answer_start'],\n",
    "                        'answer_end': answer['answer_start'] + len(answer['text'])\n",
    "                    })\n",
    "    \n",
    "    return contexts, answers\n",
    "\n",
    "train_contexts, train_answers = load_spoken_squad_data('Spoken-SQuAD-master/spoken_train-v1.1.json')\n",
    "\n",
    "def tokenize_and_align_labels(contexts, answers):\n",
    "    encodings = tokenizer(contexts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    add_token_positions(encodings, answers)\n",
    "    return encodings\n",
    "\n",
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i in range(len(answers)):\n",
    "        context = encodings['input_ids'][i]\n",
    "        answer = answers[i]['text']\n",
    "        \n",
    "        start_char = answers[i]['answer_start']\n",
    "        end_char = answers[i]['answer_end']\n",
    "\n",
    "        # Convert input ids to tokens\n",
    "        context_tokens = tokenizer.convert_ids_to_tokens(context)\n",
    "        start_token = None\n",
    "        end_token = None\n",
    "        \n",
    "        # Find token positions that match the character indices\n",
    "        char_idx = 0\n",
    "        for idx, token in enumerate(context_tokens):\n",
    "            char_idx += len(token)  # Accumulate token length to find character positions\n",
    "            if start_char <= char_idx and start_token is None:\n",
    "                start_token = idx\n",
    "            if end_char <= char_idx and end_token is None:\n",
    "                end_token = idx\n",
    "                break\n",
    "        \n",
    "        start_positions.append(start_token)\n",
    "        end_positions.append(end_token)\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "train_encodings = tokenize_and_align_labels(train_contexts, train_answers)\n",
    "\n",
    "# Define a custom Dataset class\n",
    "class QA_Dataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "train_dataset = QA_Dataset(train_encodings)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "model = AlbertForQuestionAnswering.from_pretrained('albert-base-v2')\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Function to calculate F1 score\n",
    "def calculate_f1(start_preds, end_preds, start_trues, end_trues):\n",
    "    f1_scores = []\n",
    "    for sp, ep, st, et in zip(start_preds, end_preds, start_trues, end_trues):\n",
    "        pred = (sp.item(), ep.item())\n",
    "        true = (st.item(), et.item())\n",
    "        if pred == true:\n",
    "            f1_scores.append(1.0)  # Perfect match\n",
    "        else:\n",
    "            f1_scores.append(0.0)  \n",
    "    return sum(f1_scores) / len(f1_scores) if f1_scores else 0.0\n",
    "\n",
    "EPOCHS = 3  \n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions']. to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                         start_positions=start_positions, end_positions=end_positions)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluation for F1 score\n",
    "    model.eval()\n",
    "    start_preds, end_preds = [], []\n",
    "    start_trues, end_trues = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            start_logits = outputs.start_logits\n",
    "            end_logits = outputs.end_logits\n",
    "\n",
    "            start_preds.extend(torch.argmax(start_logits, dim=1).cpu().numpy())\n",
    "            end_preds.extend(torch.argmax(end_logits, dim=1).cpu().numpy())\n",
    "            start_trues.extend(batch['start_positions'].cpu().numpy())\n",
    "            end_trues.extend(batch['end_positions'].cpu().numpy())\n",
    "\n",
    "    f1 = calculate_f1(start_preds, end_preds, start_trues, end_trues)\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {loss.item()}, F1 Score = {f1}\")\n",
    "\n",
    "\n",
    "model.save_pretrained('model/albert-finetuned-spoken-squad')\n",
    "tokenizer.save_pretrained('model_save/albert-finetuned-spoken-squad')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
